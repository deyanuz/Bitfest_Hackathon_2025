{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c14e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Example: {'bn': 'আপনার এফবি আইডি নেম বিশাল আর এইখানে মামুন কেন ?', 'rm': 'Apnar fb id name Bishal ar Ekhane mamun keno ?'}\n",
      "Validation Example: {'bn': 'ভালো করে ট্রাই করেন পাবেন..', 'rm': 'valo kore trai koren paben..'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"SKNahin/bengali-transliteration-data\")\n",
    "\n",
    "# Split the dataset into training and validation subsets (90/10 split)\n",
    "dataset_split = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "train_data = dataset_split[\"train\"]\n",
    "val_data = dataset_split[\"test\"]\n",
    "\n",
    "# Display dataset samples\n",
    "print(\"Train Example:\", train_data[0])\n",
    "print(\"Validation Example:\", val_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81b2aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 2985\n",
      "Validation Samples: 332\n",
      "Sample Tokenized Banglish (Input): [250004   5787   2322 108642   3447   9351  14851   4200    187   2751\n",
      "  34414   5568    309    311    157    705      2      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1]\n",
      "Sample Tokenized Bangla (Label): [250004  21742      6 131916   9115  57031  28555  50365   4198 145853\n",
      "  16525   6386  38084  15709  14767   4198  29784  65828    705      2\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBart50Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load mBART-50 tokenizer\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"bn_BD\")\n",
    "\n",
    "# Define data preprocessing function\n",
    "def preprocess_data(dataset, min_length=5, max_length=128):\n",
    "    \"\"\"\n",
    "    Preprocess data for sequence-to-sequence tasks.\n",
    "    Filters overly short or long sequences and tokenizes inputs/labels.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The Hugging Face dataset to preprocess.\n",
    "        min_length: Minimum sequence length (in words).\n",
    "        max_length: Maximum sequence length (in tokens).\n",
    "    \n",
    "    Returns:\n",
    "        Processed dataset with tokenized inputs and labels.\n",
    "    \"\"\"\n",
    "    inputs = []  # Store tokenized inputs\n",
    "    labels = []  # Store tokenized labels\n",
    "\n",
    "    # Iterate through dataset examples\n",
    "    for example in dataset:\n",
    "        # Extract source (Banglish) and target (Bangla) texts\n",
    "        input_text = example[\"rm\"]\n",
    "        label_text = example[\"bn\"]\n",
    "\n",
    "        # Filter overly short sentences (based on words) or excessively long\n",
    "        if len(input_text.split()) < min_length or len(label_text.split()) < min_length:\n",
    "            continue\n",
    "        if len(input_text.split()) > max_length or len(label_text.split()) > max_length:\n",
    "            continue\n",
    "\n",
    "        # Tokenize source and target texts\n",
    "        input_ids = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"np\")[\"input_ids\"].squeeze()\n",
    "        label_ids = tokenizer(label_text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"np\")[\"input_ids\"].squeeze()\n",
    "\n",
    "        inputs.append(input_ids)\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    # Return processed data as NumPy arrays\n",
    "    return np.array(inputs), np.array(labels)\n",
    "\n",
    "# Apply preprocessing to the train and validation datasets\n",
    "train_inputs, train_labels = preprocess_data(train_data, min_length=5, max_length=128)\n",
    "val_inputs, val_labels = preprocess_data(val_data, min_length=5, max_length=128)\n",
    "\n",
    "# Verify results\n",
    "print(f\"Training Samples: {len(train_inputs)}\")\n",
    "print(f\"Validation Samples: {len(val_inputs)}\")\n",
    "print(\"Sample Tokenized Banglish (Input):\", train_inputs[0])\n",
    "print(\"Sample Tokenized Bangla (Label):\", train_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfcde7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bn', 'rm']\n"
     ]
    }
   ],
   "source": [
    "print(train_data.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb1872f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "\n",
    "# Load the mBART-50 tokenizer and model\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"bn_BD\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "\n",
    "# Verify that the model is ready\n",
    "print(\"Model and Tokenizer Loaded Successfully\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b5ccbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compile() got an unexpected keyword argument 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m SparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Prepare training and validation data as TensorFlow datasets\u001b[39;00m\n\u001b[0;32m     11\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((\n\u001b[0;32m     12\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]},\n\u001b[0;32m     13\u001b[0m     train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     14\u001b[0m ))\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2994\u001b[0m, in \u001b[0;36mModule.compile\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2985\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2986\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2987\u001b[0m \u001b[38;5;124;03m    Compile this Module's forward using :func:`torch.compile`.\u001b[39;00m\n\u001b[0;32m   2988\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2992\u001b[0m \u001b[38;5;124;03m    See :func:`torch.compile` for details on the arguments for this function.\u001b[39;00m\n\u001b[0;32m   2993\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2994\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: compile() got an unexpected keyword argument 'optimizer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a custom PyTorch Dataset class\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.inputs[idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TransliterationDataset(train_inputs, train_labels)\n",
    "val_dataset = TransliterationDataset(val_inputs, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3  # Adjust based on dataset size and performance\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move inputs and labels to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move inputs and labels to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"banglish_to_bangla_model\")\n",
    "tokenizer.save_pretrained(\"banglish_to_bangla_model\")\n",
    "print(\"Model Saved Successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31383e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
